'use client';

import { useState, useRef, useEffect } from 'react';
import { Bot, Send, X, Loader2, User, ExternalLink, Calendar, Phone, Mail, MessageCircle, Mic, MicOff, Volume2, VolumeX, Settings, Play, Pause } from 'lucide-react';
import { Button } from '@/components/ui/button';
import { Card, CardHeader, CardTitle, CardContent, CardFooter } from '@/components/ui/card';
import { Input } from '@/components/ui/input';
import { ScrollArea } from '@/components/ui/scroll-area';
import { Badge } from '@/components/ui/badge';
import { useToast } from '@/hooks/use-toast';
import { useLanguage } from '@/contexts/language-context';
import emailjs from '@emailjs/browser';

import type { ChatMessage } from '@/ai/schemas/chatbot';
import { Avatar, AvatarFallback, AvatarImage } from './ui/avatar';
import { AnimatedLogo } from './ui/animated-logo';

interface RichResponse {
  text: string;
  actions?: Array<{
    label: string;
    type: 'button' | 'link' | 'contact';
    action: string;
    icon?: string;
  }>;
  leadScore?: number;
  nextQuestions?: string[];
}

interface VoiceAgentState {
  isActive: boolean;
  isListening: boolean;
  isSpeaking: boolean;
  isProcessing: boolean;
  confidence: number;
  transcript: string;
  fullTranscript: string;
}

interface ElevenLabsVoice {
  voice_id: string;
  name: string;
  category: string;
  description: string;
  labels: Record<string, string>;
}

export default function ElevenLabsVoiceChatbot() {
  const { t, language } = useLanguage();
  const [isOpen, setIsOpen] = useState(false);
  const [messages, setMessages] = useState<ChatMessage[]>([]);
  const [isInitialized, setIsInitialized] = useState(false);
  const [input, setInput] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [sessionId, setSessionId] = useState('');
  
  // Voice Agent states
  const [voiceAgent, setVoiceAgent] = useState<VoiceAgentState>({
    isActive: false,
    isListening: false,
    isSpeaking: false,
    isProcessing: false,
    confidence: 0,
    transcript: '',
    fullTranscript: ''
  });
  
  // Audio and speech states
  const [isMuted, setIsMuted] = useState(false);
  const [speechRate, setSpeechRate] = useState(1);

  
  // ElevenLabs states
  const [availableVoices, setAvailableVoices] = useState<ElevenLabsVoice[]>([]);
  const [selectedVoice, setSelectedVoice] = useState<string>('');
  const [isLoadingVoices, setIsLoadingVoices] = useState(false);
  
  // Scheduling state
  const [schedulingState, setSchedulingState] = useState<{
    isActive: boolean;
    userInfo: { name: string; email: string };
    bookingDetails?: {
      date: string;
      time: string;
      timezone: string;
      ownerTime: string;
    };
  }>({
    isActive: false,
    userInfo: { name: '', email: '' }
  });
  
  const scrollAreaRef = useRef<HTMLDivElement>(null);
  const { toast } = useToast();
  
  // Voice processing refs
  const recognitionRef = useRef<any>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const microphoneRef = useRef<MediaStreamAudioSourceNode | null>(null);
  const animationFrameRef = useRef<number | null>(null);

  // ElevenLabs API configuration
  const ELEVENLABS_API_KEY = 'sk_0319ae36d08c569341e843b46f2cb8838fb45e93373c2db3';
  const ELEVENLABS_API_URL = 'https://api.elevenlabs.io/v1';

  // Initialize voice recognition and ElevenLabs
  useEffect(() => {
    if (typeof window !== 'undefined') {
      console.log('ðŸ”Š Initializing ElevenLabs Voice Agent...');
      
      // Initialize Speech Recognition
      if ('webkitSpeechRecognition' in window) {
        console.log('âœ… Speech Recognition supported');
        const SpeechRecognition = (window as any).webkitSpeechRecognition || (window as any).SpeechRecognition;
        recognitionRef.current = new SpeechRecognition();
        
        recognitionRef.current.continuous = true;
        recognitionRef.current.interimResults = true;
        recognitionRef.current.lang = language === 'en' ? 'en-US' : 'fr-FR';
        
        recognitionRef.current.onstart = () => {
          console.log('ðŸŽ¤ Speech recognition started');
          setVoiceAgent(prev => ({ ...prev, isListening: true }));
        };
        
        recognitionRef.current.onresult = (event: any) => {
          let finalTranscript = '';
          let interimTranscript = '';
          let maxConfidence = 0;
          
          for (let i = event.resultIndex; i < event.results.length; i++) {
            const transcript = event.results[i][0].transcript;
            const confidence = event.results[i][0].confidence;
            
            if (event.results[i].isFinal) {
              finalTranscript += transcript;
              maxConfidence = Math.max(maxConfidence, confidence);
            } else {
              interimTranscript += transcript;
            }
          }
          
          setVoiceAgent(prev => ({
            ...prev,
            transcript: interimTranscript,
            confidence: maxConfidence
          }));
          
          if (finalTranscript) {
            console.log('ðŸŽ¯ Final transcript:', finalTranscript);
            handleVoiceInput(finalTranscript);
          }
        };
        
        recognitionRef.current.onerror = (event: any) => {
          console.error('âŒ Speech recognition error:', event.error);
          setVoiceAgent(prev => ({ ...prev, isListening: false }));
        };
        
        recognitionRef.current.onend = () => {
          console.log('ðŸ›‘ Speech recognition ended');
          setVoiceAgent(prev => ({ ...prev, isListening: false }));
        };
      } else {
        console.warn('âš ï¸ Speech Recognition not supported');
      }
      
      // Initialize Audio Context for visual effects
      try {
        audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)();
        console.log('ðŸŽµ Audio context initialized');
      } catch (error) {
        console.error('âŒ Failed to initialize audio context:', error);
      }
      
      // Load ElevenLabs voices immediately
      loadElevenLabsVoices();
    }
    
    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.stop();
      }
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
      }
    };
  }, [language]);

  // Load available voices from ElevenLabs
  const loadElevenLabsVoices = async () => {
    try {
      setIsLoadingVoices(true);
      console.log('ðŸŽµ Loading ElevenLabs voices...');
      console.log('ðŸ”‘ Using API key:', ELEVENLABS_API_KEY.substring(0, 10) + '...');
      console.log('ðŸŒ API URL:', ELEVENLABS_API_URL);
      
      // Set a fallback voice ID immediately for testing
      const fallbackVoiceId = '21m00Tcm4TlvDq8ikWAM'; // Default ElevenLabs voice
      setSelectedVoice(fallbackVoiceId);
      console.log('ðŸ”„ Set fallback voice ID:', fallbackVoiceId);
      
      const response = await fetch(`${ELEVENLABS_API_URL}/voices`, {
        method: 'GET',
        headers: {
          'xi-api-key': ELEVENLABS_API_KEY,
          'Content-Type': 'application/json'
        }
      });
      
      console.log('ðŸ“¡ Response status:', response.status);
      
      if (!response.ok) {
        const errorText = await response.text();
        console.error('âŒ API Error Response:', errorText);
        throw new Error(`Failed to load voices: ${response.status} - ${errorText}`);
      }
      
      const data = await response.json();
      console.log('ðŸ“„ Full API response:', data);
      
      const voices = data.voices || [];
      console.log('ðŸŽµ Available voices:', voices.length);
      
      if (voices.length === 0) {
        console.warn('âš ï¸ No voices returned from API, using fallback');
        toast({
          title: "No Voices Available",
          description: "ElevenLabs API returned no voices. Using fallback voice.",
          variant: "destructive",
        });
        return;
      }
      
      setAvailableVoices(voices);
      
      // Select a default voice (preferably English, professional)
      const defaultVoice = voices.find((voice: any) => 
        voice.labels?.language === 'en' && 
        (voice.category === 'cloned' || voice.category === 'premade')
      ) || voices[0];
      
      if (defaultVoice) {
        setSelectedVoice(defaultVoice.voice_id);
        console.log('ðŸŽ¯ Selected default voice:', defaultVoice.name, 'ID:', defaultVoice.voice_id);
      }
      
    } catch (error: any) {
      console.error('âŒ Failed to load ElevenLabs voices:', error);
      toast({
        title: "Voice Loading Error",
        description: `Failed to load voices: ${error.message}. Using fallback voice.`,
        variant: "destructive",
      });
      
      console.log('ðŸ”„ Continuing with fallback voice ID for testing');
    } finally {
      setIsLoadingVoices(false);
    }
  };

  // ElevenLabs text-to-speech function with proper autoplay handling
  const speakWithElevenLabs = async (text: string) => {
    console.log('ðŸ”Š Attempting to speak:', text);
    console.log('ðŸŽ¯ Selected voice ID:', selectedVoice);
    console.log('ðŸ”‡ Is muted:', isMuted);
    
    // Use fallback voice if none selected
    const voiceToUse = selectedVoice || '21m00Tcm4TlvDq8ikWAM';
    console.log('ðŸŽ¯ Using voice ID:', voiceToUse);
    
    if (isMuted) {
      console.log('ðŸ”‡ Speech muted, skipping');
      return;
    }
    
    try {
      console.log('ðŸ”Š Speaking with ElevenLabs:', text);
      setVoiceAgent(prev => ({ ...prev, isSpeaking: true }));
      
      const requestBody = {
        text: text,
        model_id: 'eleven_monolingual_v1',
        voice_settings: {
          stability: 0.5,
          similarity_boost: 0.5
        }
      };
      
      console.log('ðŸ“¤ Request body:', requestBody);
      console.log('ðŸ”‘ API key:', ELEVENLABS_API_KEY.substring(0, 10) + '...');
      console.log('ðŸŽ¯ Making TTS request to:', `${ELEVENLABS_API_URL}/text-to-speech/${voiceToUse}`);
      
      const response = await fetch(`${ELEVENLABS_API_URL}/text-to-speech/${voiceToUse}`, {
        method: 'POST',
        headers: {
          'xi-api-key': ELEVENLABS_API_KEY,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify(requestBody)
      });
      
      console.log('ðŸ“¡ TTS Response status:', response.status);
      
      if (!response.ok) {
        const errorText = await response.text();
        console.error('âŒ TTS API Error Response:', errorText);
        throw new Error(`ElevenLabs TTS API error: ${response.status} - ${errorText}`);
      }
      
      const audioBlob = await response.blob();
      console.log('ðŸŽµ Audio blob size:', audioBlob.size, 'bytes');
      
      const audioUrl = URL.createObjectURL(audioBlob);
      console.log('ðŸ”— Audio URL created:', audioUrl);
      
      // Play the audio with proper error handling
      const audio = new Audio(audioUrl);
      
      audio.onloadstart = () => console.log('ðŸŽµ Audio loading started');
      audio.oncanplay = () => console.log('ðŸŽµ Audio can play');
      audio.onplay = () => console.log('ðŸŽµ Audio play started');
      
      audio.onended = () => {
        console.log('âœ… ElevenLabs speech ended');
        setVoiceAgent(prev => ({ ...prev, isSpeaking: false }));
        URL.revokeObjectURL(audioUrl);
      };
      
      audio.onerror = (error) => {
        console.error('âŒ Audio playback error:', error);
        setVoiceAgent(prev => ({ ...prev, isSpeaking: false }));
        URL.revokeObjectURL(audioUrl);
      };
      
      console.log('ðŸŽ¯ Starting audio playback...');
      
      try {
        // Try to play audio with user interaction context
        const playResult = await audio.play();
        console.log('ðŸŽµ Audio play result:', playResult);
        console.log('ðŸŽ¯ ElevenLabs speech started');
      } catch (playError: any) {
        console.error('âŒ Audio play failed:', playError);
        
        if (playError.name === 'NotAllowedError') {
          // Autoplay blocked - show user instruction
          toast({
            title: "Audio Playback Blocked",
            description: "Please click the Voice Agent button again to enable audio playback.",
            variant: "destructive",
          });
          
          // Reset voice agent state
          setVoiceAgent(prev => ({ ...prev, isActive: false, isSpeaking: false }));
          if (recognitionRef.current) {
            recognitionRef.current.stop();
          }
        } else {
          throw playError;
        }
      }
      
    } catch (error: any) {
      console.error('âŒ ElevenLabs speech error:', error);
      setVoiceAgent(prev => ({ ...prev, isSpeaking: false }));
      
      toast({
        title: "Voice Error",
        description: `Failed to generate speech: ${error.message}`,
        variant: "destructive",
      });
    }
  };

  // Message storage functions
  const saveMessagesToStorage = (messages: ChatMessage[], sessionId: string) => {
    try {
      const chatData = {
        sessionId,
        messages,
        timestamp: Date.now(),
        lastUpdated: new Date().toISOString()
      };
      localStorage.setItem(`chatbot_${sessionId}`, JSON.stringify(chatData));
    } catch (error) {
      console.error('Failed to save messages to localStorage:', error);
    }
  };

  const loadMessagesFromStorage = (sessionId: string): ChatMessage[] => {
    try {
      const stored = localStorage.getItem(`chatbot_${sessionId}`);
      if (stored) {
        const chatData = JSON.parse(stored);
        return chatData.messages || [];
      }
    } catch (error) {
      console.error('Failed to load messages from localStorage:', error);
    }
    return [];
  };

  // Initialize EmailJS
  useEffect(() => {
    try {
      const publicKey = process.env.NEXT_PUBLIC_EMAILJS_PUBLIC_KEY || 'COtKbmSCzdpiCZDwB';
      emailjs.init(publicKey);
    } catch (error) {
      console.error('EmailJS initialization failed:', error);
    }
  }, []);

  useEffect(() => {
    if (!sessionId) {
      setSessionId(`session_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`);
    }
  }, [sessionId]);

  useEffect(() => {
    if (isOpen && !isInitialized) {
      const storedMessages = loadMessagesFromStorage(sessionId);
      
      if (storedMessages.length > 0) {
        setMessages(storedMessages);
      } else {
        const greetingMessage = language === 'en'
          ? "Hello! I'm Zephyr, your AI voice agent powered by ElevenLabs. I can help you with appointments, inquiries, and more. Click 'Voice Agent' to start a natural conversation!"
          : "Bonjour ! Je suis Zephyr, votre agent vocal IA alimentÃ© par ElevenLabs. Je peux vous aider avec les rendez-vous, les demandes et plus encore. Cliquez sur 'Agent Vocal' pour commencer une conversation naturelle !";
        
        setMessages([
          { role: 'model', content: greetingMessage }
        ]);
      }
      setIsInitialized(true);
    }
  }, [isOpen, isInitialized, sessionId, language]);

  // Auto-scroll to bottom
  useEffect(() => {
    const scrollToBottom = () => {
      if (scrollAreaRef.current) {
        const scrollElement = scrollAreaRef.current;
        scrollElement.scrollTop = scrollElement.scrollHeight;
      }
    };
    
    scrollToBottom();
    
    const timer1 = setTimeout(scrollToBottom, 50);
    const timer2 = setTimeout(scrollToBottom, 150);
    const timer3 = setTimeout(scrollToBottom, 300);
    
    return () => {
      clearTimeout(timer1);
      clearTimeout(timer2);
      clearTimeout(timer3);
    };
  }, [messages, isLoading]);

  // Handle voice input
  const handleVoiceInput = async (transcript: string) => {
    if (!transcript.trim()) return;
    
    // Update full transcript
    setVoiceAgent(prev => ({
      ...prev,
      fullTranscript: prev.fullTranscript + '\n' + transcript,
      transcript: '',
      isProcessing: true
    }));
    
    // Add user voice message
    const userMessage = { role: 'user' as const, content: transcript };
    const newMessages: ChatMessage[] = [...messages, userMessage];
    setMessages(newMessages);
    saveMessagesToStorage(newMessages, sessionId);
    
    // Process the voice input
    await processMessage(transcript, newMessages);
    
    setVoiceAgent(prev => ({ ...prev, isProcessing: false }));
  };

  // Process message (voice or text)
  const processMessage = async (message: string, currentMessages: ChatMessage[]) => {
    setIsLoading(true);
    
    try {
      // Check for booking intent
      const bookingKeywords = ['schedule', 'book', 'appointment', 'meeting', 'call', 'consultation', 'set up', 'arrange', 'reserve', 'slot', 'time', 'calendar'];
      const hasBookingIntent = bookingKeywords.some(keyword => 
        message.toLowerCase().includes(keyword)
      );
      
      if (hasBookingIntent) {
        await handleBookingIntent(message, currentMessages);
      } else {
        // Handle general conversation
        const response = await handleGeneralConversation(message);
        setMessages([...currentMessages, { role: 'model', content: response }]);
        
        // Speak the response if voice agent is active
        if (voiceAgent.isActive) {
          await speakWithElevenLabs(response);
        }
      }
      
      saveMessagesToStorage(messages, sessionId);
      
    } catch (error) {
      console.error('Error processing message:', error);
      const errorMessage = language === 'en' 
        ? "I'm sorry, I encountered an error processing your request. Please try again."
        : "Je suis dÃ©solÃ©, j'ai rencontrÃ© une erreur en traitant votre demande. Veuillez rÃ©essayer.";
      
      setMessages([...currentMessages, { role: 'model', content: errorMessage }]);
      
      if (voiceAgent.isActive) {
        await speakWithElevenLabs(errorMessage);
      }
    } finally {
      setIsLoading(false);
    }
  };

  // Handle booking intent
  const handleBookingIntent = async (message: string, currentMessages: ChatMessage[]) => {
    const response = language === 'en'
      ? "I'd be happy to help you schedule a consultation! I can detect that you want to book an appointment. Let me open our scheduling system for you."
      : "Je serais ravi de vous aider Ã  programmer une consultation ! Je peux dÃ©tecter que vous voulez rÃ©server un rendez-vous. Laissez-moi ouvrir notre systÃ¨me de planification pour vous.";
    
    setMessages([...currentMessages, { role: 'model', content: response }]);
    
    if (voiceAgent.isActive) {
      await speakWithElevenLabs(response);
    }
    

    
    setSchedulingState(prev => ({
      ...prev,
      isActive: true
    }));
  };

  // Handle general conversation
  const handleGeneralConversation = async (message: string): Promise<string> => {
    const lowerMessage = message.toLowerCase();
    
    if (lowerMessage.includes('hello') || lowerMessage.includes('hi') || lowerMessage.includes('hey')) {
      return language === 'en' 
        ? "Hello! How can I help you today? You can ask me about our services, schedule a consultation, or just chat naturally."
        : "Bonjour ! Comment puis-je vous aider aujourd'hui ? Vous pouvez me demander nos services, programmer une consultation ou simplement discuter naturellement.";
    }
    
    if (lowerMessage.includes('services') || lowerMessage.includes('what do you do') || lowerMessage.includes('cognivex')) {
      return language === 'en'
        ? "CognivexAI - Premier AI Solutions Partner\n\nðŸ“Š Data Analysis\n   â€¢ Automated insights & reporting\n   â€¢ Business intelligence dashboards\n   â€¢ Predictive analytics\n\nðŸ¤– AI Chatbots\n   â€¢ 24/7 intelligent support\n   â€¢ Lead qualification & booking\n   â€¢ CRM integration\n\nðŸŒ AI-Enhanced Websites\n   â€¢ Modern responsive design\n   â€¢ AI personalization\n   â€¢ Chatbot integration\n\nðŸ’Ž Success Metrics\n   â€¢ 40% cost reduction\n   â€¢ 3x faster decisions\n   â€¢ 95% accuracy\n\nWhich service interests you most?"
        : "CognivexAI - Partenaire de Solutions IA de Premier Plan\n\nðŸ“Š Analyse de DonnÃ©es\n   â€¢ Insights et rapports automatisÃ©s\n   â€¢ Tableaux de bord d'intelligence d'affaires\n   â€¢ Analytique prÃ©dictive\n\nðŸ¤– Chatbots IA\n   â€¢ Support intelligent 24h/24\n   â€¢ Qualification des prospects et rÃ©servation\n   â€¢ IntÃ©gration CRM\n\nðŸŒ Sites Web AmÃ©liorÃ©s par l'IA\n   â€¢ Design responsive moderne\n   â€¢ Personnalisation IA\n   â€¢ IntÃ©gration chatbot\n\nðŸ’Ž MÃ©triques de SuccÃ¨s\n   â€¢ 40% de rÃ©duction des coÃ»ts\n   â€¢ DÃ©cisions 3x plus rapides\n   â€¢ 95% de prÃ©cision\n\nQuel service vous intÃ©resse le plus ?";
    }
    
    if (lowerMessage.includes('contact') || lowerMessage.includes('email') || lowerMessage.includes('phone')) {
      return language === 'en'
        ? "You can reach us at snazzy.mugi@gmail.com or schedule a call through me. Would you like me to help you book a consultation?"
        : "Vous pouvez nous joindre Ã  snazzy.mugi@gmail.com ou programmer un appel par mon intermÃ©diaire. Souhaitez-vous que je vous aider Ã  rÃ©server une consultation ?";
    }
    
    return language === 'en'
      ? "That's interesting! I'd be happy to help you with that. You can ask me about our services, schedule a consultation, or just continue our conversation naturally."
      : "C'est intÃ©ressant ! Je serais ravi de vous aider avec cela. Vous pouvez me demander nos services, programmer une consultation ou simplement continuer notre conversation naturellement.";
  };

  // Toggle voice agent mode with proper autoplay handling
  const toggleVoiceAgent = () => {
    const newState = !voiceAgent.isActive;
    setVoiceAgent(prev => ({ ...prev, isActive: newState }));
    
    if (newState) {
      // Start listening immediately
      if (recognitionRef.current) {
        recognitionRef.current.start();
      }
      
      // Speak welcome message after user interaction
      const welcomeMessage = language === 'en'
        ? "Voice Agent mode activated. I'm now listening and ready to help you with professional ElevenLabs voice. You can speak naturally and I'll respond with voice."
        : "Mode Agent Vocal activÃ©. J'Ã©coute maintenant et je suis prÃªt Ã  vous aider avec la voix professionnelle ElevenLabs. Vous pouvez parler naturellement et je rÃ©pondrai par la voix.";
      
      // Use setTimeout to ensure user interaction context
      setTimeout(() => {
        speakWithElevenLabs(welcomeMessage);
      }, 100);
    } else {
      // Stop listening and speaking
      if (recognitionRef.current) {
        recognitionRef.current.stop();
      }
      setVoiceAgent(prev => ({ 
        ...prev, 
        isListening: false, 
        isSpeaking: false,
        transcript: '',
        fullTranscript: ''
      }));
    }
  };

  // Handle form submission
  const handleSendMessage = async (e: React.FormEvent) => {
    e.preventDefault();
    if (!input.trim() || isLoading) return;

    const userMessage = { role: 'user' as const, content: input };
    const newMessages: ChatMessage[] = [...messages, userMessage];
    setMessages(newMessages);
    saveMessagesToStorage(newMessages, sessionId);
    
    setInput('');
    await processMessage(input, newMessages);
  };

  // Handle action clicks
  const handleActionClick = (action: string) => {
    setInput(action);
    setTimeout(() => {
      const userMessage = { role: 'user' as const, content: action };
      const newMessages: ChatMessage[] = [...messages, userMessage];
      setMessages(newMessages);
      saveMessagesToStorage(newMessages, sessionId);
      processMessage(action, newMessages);
    }, 100);
  };

  // Render rich response
  const renderRichResponse = (message: ChatMessage, index: number) => {
    if (message.role === 'model') {
      return (
        <div key={index} className="flex items-start gap-3">
          <Avatar className="h-8 w-8 bg-gradient-to-r from-orange-500 to-orange-600">
            <AvatarFallback className="text-white font-semibold">Z</AvatarFallback>
          </Avatar>
          <div className="flex-1 space-y-2">
            <div className="rounded-lg px-3 py-2 bg-black/80 backdrop-blur-sm border border-gray-700/50">
              <p className="text-sm text-gray-200">{message.content}</p>
            </div>
            
            {/* Quick Actions */}
            {index === messages.length - 1 && !isLoading && (
              <div className="flex flex-wrap gap-2 pt-2">
                <Button
                  size="sm"
                  variant="outline"
                  className="text-xs h-7 px-3 bg-gray-800/50 border-gray-600 hover:bg-orange-500/20 hover:border-orange-500/50 text-gray-300 hover:text-orange-300"
                  onClick={() => handleActionClick("I'd like to schedule a consultation")}
                >
                  <Calendar className="h-3 w-3 mr-1" />
                  Schedule Call
                </Button>
                <Button
                  size="sm"
                  variant="outline"
                  className="text-xs h-7 px-3 bg-gray-800/50 border-gray-600 hover:bg-orange-500/20 hover:border-orange-500/50 text-gray-300 hover:text-orange-300"
                  onClick={() => handleActionClick("What are your contact details?")}
                >
                  <Mail className="h-3 w-3 mr-1" />
                  Contact Info
                </Button>
                <Button
                  size="sm"
                  variant="outline"
                  className="text-xs h-7 px-3 bg-gray-800/50 border-gray-600 hover:bg-orange-500/20 hover:border-orange-500/50 text-gray-300 hover:text-orange-300"
                  onClick={() => handleActionClick("Tell me about your services")}
                >
                  <MessageCircle className="h-3 w-3 mr-1" />
                  Services
                </Button>
              </div>
            )}
          </div>
        </div>
      );
    }
    
    if (message.role === 'user') {
      return (
        <div key={index} className="flex items-start gap-3 justify-end">
          <div className="flex-1 max-w-[80%]">
            <div className="rounded-lg px-3 py-2 bg-blue-500/20 text-blue-100 text-sm">
              {message.content}
            </div>
          </div>
          <Avatar className="h-8 w-8 bg-gradient-to-r from-gray-600 to-gray-700">
            <AvatarFallback className="text-white"><User className="h-4 w-4" /></AvatarFallback>
          </Avatar>
        </div>
      );
    }
    
    return null;
  };

  return (
    <>
      <div className="fixed bottom-6 right-6 z-50">
        {/* Voice Agent Status Indicator */}
        {voiceAgent.isActive && (
          <div className="absolute bottom-12 md:bottom-20 right-0 bg-gradient-to-r from-orange-500 to-red-500 backdrop-blur-sm rounded-lg px-3 py-2 border border-orange-400 shadow-lg animate-pulse">
            <div className="flex items-center space-x-2">
              <Mic className="h-4 w-4 text-white animate-pulse" />
              <p className="text-white text-sm font-medium">
                {voiceAgent.isListening ? 'Listening...' : voiceAgent.isSpeaking ? 'Speaking...' : 'Voice Agent Active'}
              </p>
            </div>
          </div>
        )}
        
        {/* Simple message bubble above the chatbot icon */}
        {!isOpen && (
          <div className="absolute bottom-12 md:bottom-20 right-0 bg-black/90 backdrop-blur-sm rounded-lg px-2 md:px-3 py-1.5 md:py-2 border border-gray-700 shadow-lg animate-in slide-in-from-bottom-2 duration-300">
            <p className="text-white text-xs md:text-sm font-medium whitespace-nowrap">
              {t('chatbot.greeting')}
            </p>
            <div className="absolute top-full right-3 md:right-4 w-2 h-2 bg-black/90 border-r border-b border-gray-700 transform rotate-45"></div>
          </div>
        )}
        
        <Button onClick={() => setIsOpen(!isOpen)} size="icon" className="rounded-full h-16 w-16 md:h-18 md:w-18 shadow-lg flex items-center justify-center bg-black hover:bg-gray-800">
          {isOpen ? <X className="h-8 w-8 md:h-9 md:w-9 text-white" /> : <AnimatedLogo className="h-10 w-10 md:h-12 md:w-12" />}
          <span className="sr-only">{isOpen ? 'Close chat' : 'Open chat'}</span>
        </Button>
      </div>
      
      {isOpen && (
        <Card className={`fixed bottom-24 right-6 z-50 w-[calc(100vw-3rem)] md:w-96 h-96 md:h-[500px] flex flex-col rounded-2xl animate-in slide-in-from-bottom-10 fade-in-50 duration-300 transition-all duration-500 ${
          voiceAgent.isActive 
            ? 'ring-4 ring-orange-500/50 shadow-2xl shadow-orange-500/25 bg-gradient-to-br from-gray-900/95 to-gray-800/95' 
            : 'bg-black/90 backdrop-blur-lg border border-gray-700/50 shadow-2xl'
        }`}>
          <CardHeader className="flex flex-row items-center justify-between p-3 border-b border-gray-700/50">
            <div className="flex items-center space-x-3">
              <Avatar className="h-8 w-8 bg-gradient-to-r from-orange-500 to-orange-600">
                <AvatarFallback className="text-white font-semibold">Z</AvatarFallback>
              </Avatar>
              <div>
                <CardTitle className="font-headline text-base text-white">Zephyr AI</CardTitle>
                <p className="text-xs text-orange-300">ElevenLabs Powered</p>
              </div>
            </div>
            
            {/* Voice Agent Controls */}
            <div className="flex items-center space-x-2">
              <Button
                variant="ghost"
                size="icon"
                onClick={toggleVoiceAgent}
                className={`h-8 w-8 transition-all duration-300 ${
                  voiceAgent.isActive 
                    ? 'text-orange-400 bg-orange-500/20 border border-orange-500/50 animate-pulse' 
                    : 'text-gray-400 hover:text-orange-400 hover:bg-orange-500/20'
                }`}
                title={voiceAgent.isActive ? 'Voice Agent Active' : 'Activate Voice Agent'}
              >
                <Mic className="h-4 w-4" />
              </Button>
              
              {voiceAgent.isActive && (
                <Button
                  variant="ghost"
                  size="icon"
                  onClick={() => setIsMuted(!isMuted)}
                  className={`h-8 w-8 ${isMuted ? 'text-red-400' : 'text-green-400'}`}
                  title={isMuted ? 'Unmute' : 'Mute'}
                >
                  {isMuted ? <VolumeX className="h-4 w-4" /> : <Volume2 className="h-4 w-4" />}
                </Button>
              )}
              
              <Button variant="ghost" size="icon" onClick={() => setIsOpen(false)} className="h-8 w-8 hover:bg-orange-500/20">
                <X className="h-4 w-4 text-gray-400" />
                <span className="sr-only">Close</span>
              </Button>
            </div>
          </CardHeader>
          
          {/* Voice Agent Status Bar */}
          {voiceAgent.isActive && (
            <div className="px-3 py-2 bg-gradient-to-r from-orange-500/20 to-red-500/20 border-b border-orange-500/30">
              <div className="flex items-center justify-between text-xs">
                <div className="flex items-center space-x-2">
                  <div className={`w-2 h-2 rounded-full ${voiceAgent.isListening ? 'bg-green-400 animate-pulse' : 'bg-gray-400'}`}></div>
                  <span className="text-orange-300">
                    {voiceAgent.isListening ? 'Listening' : voiceAgent.isSpeaking ? 'Speaking...' : 'Ready'}
                  </span>
                </div>
                <div className="flex items-center space-x-2">
                  <span className="text-orange-300">Confidence: {Math.round(voiceAgent.confidence * 100)}%</span>
                </div>
              </div>
              
              {/* Live Transcript */}
              {voiceAgent.transcript && (
                <div className="mt-2 p-2 bg-black/30 rounded text-xs text-gray-300">
                  <span className="text-orange-400">Live: </span>
                  {voiceAgent.transcript}
                </div>
              )}
            </div>
          )}
          
          <div className="flex-1 p-3 overflow-y-auto" ref={scrollAreaRef}>
            <div className="space-y-4">
              {messages.map((message, index) => renderRichResponse(message, index))}
              {isLoading && (
                <div className="flex items-start gap-3">
                   <Avatar className="h-8 w-8 bg-gradient-to-r from-orange-500 to-orange-600">
                      <AvatarFallback className="text-white font-semibold">Z</AvatarFallback>
                    </Avatar>
                  <div className="rounded-lg px-3 py-2 bg-black/80 backdrop-blur-sm border border-gray-700/50">
                    <div className="flex items-center gap-2">
                      <Loader2 className="h-4 w-4 animate-spin text-orange-400" />
                      <span className="text-sm text-gray-300">{t('chatbot.thinking')}</span>
                    </div>
                  </div>
                </div>
              )}
            </div>
          </div>
          

          
          <CardFooter className="p-3 border-t border-gray-700/50">
            <form onSubmit={handleSendMessage} data-chatbot-form="true" className="flex w-full items-center space-x-2">
              <Input
                value={input}
                onChange={(e) => setInput(e.target.value)}
                placeholder={voiceAgent.isActive ? "Type or speak naturally..." : "Type your message..."}
                autoComplete="off"
                disabled={isLoading}
                className="h-10 text-sm bg-black/80 backdrop-blur-sm border border-gray-600 focus:border-orange-500/50 focus:ring-orange-500/20 text-white placeholder-gray-400"
              />
              <Button 
                type="submit" 
                size="icon" 
                disabled={isLoading || !input.trim()} 
                className="h-10 w-10 bg-gradient-to-r from-orange-500 to-orange-600 hover:from-orange-600 hover:to-orange-700 border border-orange-400/30"
              >
                <Send className="h-4 w-4 text-white" />
                <span className="sr-only">{t('chatbot.send')}</span>
              </Button>
            </form>
          </CardFooter>
        </Card>
      )}
    </>
  );
}
